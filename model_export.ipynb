{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function export in module torch.onnx:\n",
      "\n",
      "export(model, args, f, export_params=True, verbose=False, training=False, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=False, example_outputs=None, strip_doc_string=True, dynamic_axes=None)\n",
      "    Export a model into ONNX format.  This exporter runs your model\n",
      "    once in order to get a trace of its execution to be exported;\n",
      "    at the moment, it supports a limited set of dynamic models (e.g., RNNs.)\n",
      "    See also: :ref:`onnx-export`\n",
      "    Arguments:\n",
      "        model (torch.nn.Module): the model to be exported.\n",
      "        args (tuple of arguments): the inputs to\n",
      "            the model, e.g., such that ``model(*args)`` is a valid\n",
      "            invocation of the model.  Any non-Tensor arguments will\n",
      "            be hard-coded into the exported model; any Tensor arguments\n",
      "            will become inputs of the exported model, in the order they\n",
      "            occur in args.  If args is a Tensor, this is equivalent\n",
      "            to having called it with a 1-ary tuple of that Tensor.\n",
      "            (Note: passing keyword arguments to the model is not currently\n",
      "            supported.  Give us a shout if you need it.)\n",
      "        f: a file-like object (has to implement fileno that returns a file descriptor)\n",
      "            or a string containing a file name.  A binary Protobuf will be written\n",
      "            to this file.\n",
      "        export_params (bool, default True): if specified, all parameters will\n",
      "            be exported.  Set this to False if you want to export an untrained model.\n",
      "            In this case, the exported model will first take all of its parameters\n",
      "            as arguments, the ordering as specified by ``model.state_dict().values()``\n",
      "        verbose (bool, default False): if specified, we will print out a debug\n",
      "            description of the trace being exported.\n",
      "        training (bool, default False): export the model in training mode.  At\n",
      "            the moment, ONNX is oriented towards exporting models for inference\n",
      "            only, so you will generally not need to set this to True.\n",
      "        input_names(list of strings, default empty list): names to assign to the\n",
      "            input nodes of the graph, in order\n",
      "        output_names(list of strings, default empty list): names to assign to the\n",
      "            output nodes of the graph, in order\n",
      "        aten (bool, default False): [DEPRECATED. use operator_export_type] export the\n",
      "            model in aten mode. If using aten mode, all the ops original exported\n",
      "            by the functions in symbolic_opset<version>.py are exported as ATen ops.\n",
      "        export_raw_ir (bool, default False): [DEPRECATED. use operator_export_type]\n",
      "            export the internal IR directly instead of converting it to ONNX ops.\n",
      "        operator_export_type (enum, default OperatorExportTypes.ONNX):\n",
      "            OperatorExportTypes.ONNX: all ops are exported as regular ONNX ops.\n",
      "            OperatorExportTypes.ONNX_ATEN: all ops are exported as ATen ops.\n",
      "            OperatorExportTypes.ONNX_ATEN_FALLBACK: if symbolic is missing,\n",
      "                                                    fall back on ATen op.\n",
      "            OperatorExportTypes.RAW: export raw ir.\n",
      "        opset_version (int, default is 9): by default we export the model to the\n",
      "            opset version of the onnx submodule. Since ONNX's latest opset may\n",
      "            evolve before next stable release, by default we export to one stable\n",
      "            opset version. Right now, supported stable opset version is 9.\n",
      "            The opset_version must be _onnx_master_opset or in _onnx_stable_opsets\n",
      "            which are defined in torch/onnx/symbolic_helper.py\n",
      "        do_constant_folding (bool, default False): If True, the constant-folding\n",
      "            optimization is applied to the model during export. Constant-folding\n",
      "            optimization will replace some of the ops that have all constant\n",
      "            inputs, with pre-computed constant nodes.\n",
      "        example_outputs (tuple of Tensors, default None): example_outputs must be provided\n",
      "            when exporting a ScriptModule or TorchScript Function.\n",
      "        strip_doc_string (bool, default True): if True, strips the field\n",
      "            \"doc_string\" from the exported model, which information about the stack\n",
      "            trace.\n",
      "        example_outputs: example outputs of the model that is being exported.\n",
      "        dynamic_axes (dict<string, dict<int, string>> or dict<string, list(int)>, default empty dict):\n",
      "            a dictionary to specify dynamic axes of input/output, such that:\n",
      "            - KEY:  input and/or output names\n",
      "            - VALUE: index of dynamic axes for given key and potentially the name to be used for\n",
      "            exported dynamic axes. In general the value is defined according to one of the following\n",
      "            ways or a combination of both:\n",
      "            (1). A list of integers specifiying the dynamic axes of provided input. In this scenario\n",
      "            automated names will be generated and applied to dynamic axes of provided input/output\n",
      "            during export.\n",
      "            OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\n",
      "            corresponding input/output TO the name that is desired to be applied on such axis of\n",
      "            such input/output during export.\n",
      "            Example. if we have the following shape for inputs and outputs:\n",
      "                shape(input_1) = ('b', 3, 'w', 'h')\n",
      "                and shape(input_2) = ('b', 4)\n",
      "                and shape(output)  = ('b', 'd', 5)\n",
      "    \n",
      "            Then dynamic axes can be defined either as:\n",
      "                (a). ONLY INDICES:\n",
      "                    dynamic_axes = {'input_1':[0, 2, 3], 'input_2':[0], 'output':[0, 1]}\n",
      "    \n",
      "                    where automatic names will be generated for exported dynamic axes\n",
      "    \n",
      "                (b). INDICES WITH CORRESPONDING NAMES:\n",
      "                    dynamic_axes = {'input_1':{0:'batch', 1:'width', 2:'height'},\n",
      "                    'input_2':{0:'batch'},\n",
      "                    'output':{0:'batch', 1:'detections'}\n",
      "    \n",
      "                    where provided names will be applied to exported dynamic axes\n",
      "    \n",
      "                (c). MIXED MODE OF (a) and (b)\n",
      "                    dynamic_axes = {'input_1':[0, 2, 3], 'input_2':{0:'batch'}, 'output':[0,1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.onnx\n",
    "help(torch.onnx.export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenEfficientNet(\n",
      "  (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): DepthwiseSeparableConv(\n",
      "        (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "        (bn2): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "        (bn2): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(18, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "        (bn2): BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(120, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(30, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "        (bn2): BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(120, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(30, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(200, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "        (bn2): BatchNorm2d(200, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "        (bn2): BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "        (bn2): BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_pwl): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): ConvBnAct(\n",
      "        (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_head): Conv2d(960, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (global_pool): SelectAdaptivePool2d (output_size=1, pool_type=avg)\n",
      "  (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "m = timm.create_model('mobilenetv3_100', pretrained=True)\n",
    "print(m.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Obtain your model, it can be also constructed in your script explicitly\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# Invoke export\n",
    "torch.onnx.export(m, dummy_input, \"mobile.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %input.1[FLOAT, 1x3x224x224]\n",
      ") initializers (\n",
      "  %conv_stem.weight[FLOAT, 16x3x3x3]\n",
      "  %bn1.weight[FLOAT, 16]\n",
      "  %bn1.bias[FLOAT, 16]\n",
      "  %bn1.running_mean[FLOAT, 16]\n",
      "  %bn1.running_var[FLOAT, 16]\n",
      "  %bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.0.0.conv_dw.weight[FLOAT, 16x1x3x3]\n",
      "  %blocks.0.0.bn1.weight[FLOAT, 16]\n",
      "  %blocks.0.0.bn1.bias[FLOAT, 16]\n",
      "  %blocks.0.0.bn1.running_mean[FLOAT, 16]\n",
      "  %blocks.0.0.bn1.running_var[FLOAT, 16]\n",
      "  %blocks.0.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.0.0.conv_pw.weight[FLOAT, 16x16x1x1]\n",
      "  %blocks.0.0.bn2.weight[FLOAT, 16]\n",
      "  %blocks.0.0.bn2.bias[FLOAT, 16]\n",
      "  %blocks.0.0.bn2.running_mean[FLOAT, 16]\n",
      "  %blocks.0.0.bn2.running_var[FLOAT, 16]\n",
      "  %blocks.0.0.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.1.0.conv_pw.weight[FLOAT, 64x16x1x1]\n",
      "  %blocks.1.0.bn1.weight[FLOAT, 64]\n",
      "  %blocks.1.0.bn1.bias[FLOAT, 64]\n",
      "  %blocks.1.0.bn1.running_mean[FLOAT, 64]\n",
      "  %blocks.1.0.bn1.running_var[FLOAT, 64]\n",
      "  %blocks.1.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.1.0.conv_dw.weight[FLOAT, 64x1x3x3]\n",
      "  %blocks.1.0.bn2.weight[FLOAT, 64]\n",
      "  %blocks.1.0.bn2.bias[FLOAT, 64]\n",
      "  %blocks.1.0.bn2.running_mean[FLOAT, 64]\n",
      "  %blocks.1.0.bn2.running_var[FLOAT, 64]\n",
      "  %blocks.1.0.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.1.0.conv_pwl.weight[FLOAT, 24x64x1x1]\n",
      "  %blocks.1.0.bn3.weight[FLOAT, 24]\n",
      "  %blocks.1.0.bn3.bias[FLOAT, 24]\n",
      "  %blocks.1.0.bn3.running_mean[FLOAT, 24]\n",
      "  %blocks.1.0.bn3.running_var[FLOAT, 24]\n",
      "  %blocks.1.0.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.1.1.conv_pw.weight[FLOAT, 72x24x1x1]\n",
      "  %blocks.1.1.bn1.weight[FLOAT, 72]\n",
      "  %blocks.1.1.bn1.bias[FLOAT, 72]\n",
      "  %blocks.1.1.bn1.running_mean[FLOAT, 72]\n",
      "  %blocks.1.1.bn1.running_var[FLOAT, 72]\n",
      "  %blocks.1.1.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.1.1.conv_dw.weight[FLOAT, 72x1x3x3]\n",
      "  %blocks.1.1.bn2.weight[FLOAT, 72]\n",
      "  %blocks.1.1.bn2.bias[FLOAT, 72]\n",
      "  %blocks.1.1.bn2.running_mean[FLOAT, 72]\n",
      "  %blocks.1.1.bn2.running_var[FLOAT, 72]\n",
      "  %blocks.1.1.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.1.1.conv_pwl.weight[FLOAT, 24x72x1x1]\n",
      "  %blocks.1.1.bn3.weight[FLOAT, 24]\n",
      "  %blocks.1.1.bn3.bias[FLOAT, 24]\n",
      "  %blocks.1.1.bn3.running_mean[FLOAT, 24]\n",
      "  %blocks.1.1.bn3.running_var[FLOAT, 24]\n",
      "  %blocks.1.1.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.0.conv_pw.weight[FLOAT, 72x24x1x1]\n",
      "  %blocks.2.0.bn1.weight[FLOAT, 72]\n",
      "  %blocks.2.0.bn1.bias[FLOAT, 72]\n",
      "  %blocks.2.0.bn1.running_mean[FLOAT, 72]\n",
      "  %blocks.2.0.bn1.running_var[FLOAT, 72]\n",
      "  %blocks.2.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.0.conv_dw.weight[FLOAT, 72x1x5x5]\n",
      "  %blocks.2.0.bn2.weight[FLOAT, 72]\n",
      "  %blocks.2.0.bn2.bias[FLOAT, 72]\n",
      "  %blocks.2.0.bn2.running_mean[FLOAT, 72]\n",
      "  %blocks.2.0.bn2.running_var[FLOAT, 72]\n",
      "  %blocks.2.0.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.0.se.conv_reduce.weight[FLOAT, 18x72x1x1]\n",
      "  %blocks.2.0.se.conv_reduce.bias[FLOAT, 18]\n",
      "  %blocks.2.0.se.conv_expand.weight[FLOAT, 72x18x1x1]\n",
      "  %blocks.2.0.se.conv_expand.bias[FLOAT, 72]\n",
      "  %blocks.2.0.conv_pwl.weight[FLOAT, 40x72x1x1]\n",
      "  %blocks.2.0.bn3.weight[FLOAT, 40]\n",
      "  %blocks.2.0.bn3.bias[FLOAT, 40]\n",
      "  %blocks.2.0.bn3.running_mean[FLOAT, 40]\n",
      "  %blocks.2.0.bn3.running_var[FLOAT, 40]\n",
      "  %blocks.2.0.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.1.conv_pw.weight[FLOAT, 120x40x1x1]\n",
      "  %blocks.2.1.bn1.weight[FLOAT, 120]\n",
      "  %blocks.2.1.bn1.bias[FLOAT, 120]\n",
      "  %blocks.2.1.bn1.running_mean[FLOAT, 120]\n",
      "  %blocks.2.1.bn1.running_var[FLOAT, 120]\n",
      "  %blocks.2.1.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.1.conv_dw.weight[FLOAT, 120x1x5x5]\n",
      "  %blocks.2.1.bn2.weight[FLOAT, 120]\n",
      "  %blocks.2.1.bn2.bias[FLOAT, 120]\n",
      "  %blocks.2.1.bn2.running_mean[FLOAT, 120]\n",
      "  %blocks.2.1.bn2.running_var[FLOAT, 120]\n",
      "  %blocks.2.1.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.1.se.conv_reduce.weight[FLOAT, 30x120x1x1]\n",
      "  %blocks.2.1.se.conv_reduce.bias[FLOAT, 30]\n",
      "  %blocks.2.1.se.conv_expand.weight[FLOAT, 120x30x1x1]\n",
      "  %blocks.2.1.se.conv_expand.bias[FLOAT, 120]\n",
      "  %blocks.2.1.conv_pwl.weight[FLOAT, 40x120x1x1]\n",
      "  %blocks.2.1.bn3.weight[FLOAT, 40]\n",
      "  %blocks.2.1.bn3.bias[FLOAT, 40]\n",
      "  %blocks.2.1.bn3.running_mean[FLOAT, 40]\n",
      "  %blocks.2.1.bn3.running_var[FLOAT, 40]\n",
      "  %blocks.2.1.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.2.conv_pw.weight[FLOAT, 120x40x1x1]\n",
      "  %blocks.2.2.bn1.weight[FLOAT, 120]\n",
      "  %blocks.2.2.bn1.bias[FLOAT, 120]\n",
      "  %blocks.2.2.bn1.running_mean[FLOAT, 120]\n",
      "  %blocks.2.2.bn1.running_var[FLOAT, 120]\n",
      "  %blocks.2.2.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.2.conv_dw.weight[FLOAT, 120x1x5x5]\n",
      "  %blocks.2.2.bn2.weight[FLOAT, 120]\n",
      "  %blocks.2.2.bn2.bias[FLOAT, 120]\n",
      "  %blocks.2.2.bn2.running_mean[FLOAT, 120]\n",
      "  %blocks.2.2.bn2.running_var[FLOAT, 120]\n",
      "  %blocks.2.2.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.2.2.se.conv_reduce.weight[FLOAT, 30x120x1x1]\n",
      "  %blocks.2.2.se.conv_reduce.bias[FLOAT, 30]\n",
      "  %blocks.2.2.se.conv_expand.weight[FLOAT, 120x30x1x1]\n",
      "  %blocks.2.2.se.conv_expand.bias[FLOAT, 120]\n",
      "  %blocks.2.2.conv_pwl.weight[FLOAT, 40x120x1x1]\n",
      "  %blocks.2.2.bn3.weight[FLOAT, 40]\n",
      "  %blocks.2.2.bn3.bias[FLOAT, 40]\n",
      "  %blocks.2.2.bn3.running_mean[FLOAT, 40]\n",
      "  %blocks.2.2.bn3.running_var[FLOAT, 40]\n",
      "  %blocks.2.2.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.0.conv_pw.weight[FLOAT, 240x40x1x1]\n",
      "  %blocks.3.0.bn1.weight[FLOAT, 240]\n",
      "  %blocks.3.0.bn1.bias[FLOAT, 240]\n",
      "  %blocks.3.0.bn1.running_mean[FLOAT, 240]\n",
      "  %blocks.3.0.bn1.running_var[FLOAT, 240]\n",
      "  %blocks.3.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.0.conv_dw.weight[FLOAT, 240x1x3x3]\n",
      "  %blocks.3.0.bn2.weight[FLOAT, 240]\n",
      "  %blocks.3.0.bn2.bias[FLOAT, 240]\n",
      "  %blocks.3.0.bn2.running_mean[FLOAT, 240]\n",
      "  %blocks.3.0.bn2.running_var[FLOAT, 240]\n",
      "  %blocks.3.0.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.0.conv_pwl.weight[FLOAT, 80x240x1x1]\n",
      "  %blocks.3.0.bn3.weight[FLOAT, 80]\n",
      "  %blocks.3.0.bn3.bias[FLOAT, 80]\n",
      "  %blocks.3.0.bn3.running_mean[FLOAT, 80]\n",
      "  %blocks.3.0.bn3.running_var[FLOAT, 80]\n",
      "  %blocks.3.0.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.1.conv_pw.weight[FLOAT, 200x80x1x1]\n",
      "  %blocks.3.1.bn1.weight[FLOAT, 200]\n",
      "  %blocks.3.1.bn1.bias[FLOAT, 200]\n",
      "  %blocks.3.1.bn1.running_mean[FLOAT, 200]\n",
      "  %blocks.3.1.bn1.running_var[FLOAT, 200]\n",
      "  %blocks.3.1.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.1.conv_dw.weight[FLOAT, 200x1x3x3]\n",
      "  %blocks.3.1.bn2.weight[FLOAT, 200]\n",
      "  %blocks.3.1.bn2.bias[FLOAT, 200]\n",
      "  %blocks.3.1.bn2.running_mean[FLOAT, 200]\n",
      "  %blocks.3.1.bn2.running_var[FLOAT, 200]\n",
      "  %blocks.3.1.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.1.conv_pwl.weight[FLOAT, 80x200x1x1]\n",
      "  %blocks.3.1.bn3.weight[FLOAT, 80]\n",
      "  %blocks.3.1.bn3.bias[FLOAT, 80]\n",
      "  %blocks.3.1.bn3.running_mean[FLOAT, 80]\n",
      "  %blocks.3.1.bn3.running_var[FLOAT, 80]\n",
      "  %blocks.3.1.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.2.conv_pw.weight[FLOAT, 184x80x1x1]\n",
      "  %blocks.3.2.bn1.weight[FLOAT, 184]\n",
      "  %blocks.3.2.bn1.bias[FLOAT, 184]\n",
      "  %blocks.3.2.bn1.running_mean[FLOAT, 184]\n",
      "  %blocks.3.2.bn1.running_var[FLOAT, 184]\n",
      "  %blocks.3.2.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.2.conv_dw.weight[FLOAT, 184x1x3x3]\n",
      "  %blocks.3.2.bn2.weight[FLOAT, 184]\n",
      "  %blocks.3.2.bn2.bias[FLOAT, 184]\n",
      "  %blocks.3.2.bn2.running_mean[FLOAT, 184]\n",
      "  %blocks.3.2.bn2.running_var[FLOAT, 184]\n",
      "  %blocks.3.2.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.2.conv_pwl.weight[FLOAT, 80x184x1x1]\n",
      "  %blocks.3.2.bn3.weight[FLOAT, 80]\n",
      "  %blocks.3.2.bn3.bias[FLOAT, 80]\n",
      "  %blocks.3.2.bn3.running_mean[FLOAT, 80]\n",
      "  %blocks.3.2.bn3.running_var[FLOAT, 80]\n",
      "  %blocks.3.2.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.3.conv_pw.weight[FLOAT, 184x80x1x1]\n",
      "  %blocks.3.3.bn1.weight[FLOAT, 184]\n",
      "  %blocks.3.3.bn1.bias[FLOAT, 184]\n",
      "  %blocks.3.3.bn1.running_mean[FLOAT, 184]\n",
      "  %blocks.3.3.bn1.running_var[FLOAT, 184]\n",
      "  %blocks.3.3.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.3.conv_dw.weight[FLOAT, 184x1x3x3]\n",
      "  %blocks.3.3.bn2.weight[FLOAT, 184]\n",
      "  %blocks.3.3.bn2.bias[FLOAT, 184]\n",
      "  %blocks.3.3.bn2.running_mean[FLOAT, 184]\n",
      "  %blocks.3.3.bn2.running_var[FLOAT, 184]\n",
      "  %blocks.3.3.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.3.3.conv_pwl.weight[FLOAT, 80x184x1x1]\n",
      "  %blocks.3.3.bn3.weight[FLOAT, 80]\n",
      "  %blocks.3.3.bn3.bias[FLOAT, 80]\n",
      "  %blocks.3.3.bn3.running_mean[FLOAT, 80]\n",
      "  %blocks.3.3.bn3.running_var[FLOAT, 80]\n",
      "  %blocks.3.3.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.4.0.conv_pw.weight[FLOAT, 480x80x1x1]\n",
      "  %blocks.4.0.bn1.weight[FLOAT, 480]\n",
      "  %blocks.4.0.bn1.bias[FLOAT, 480]\n",
      "  %blocks.4.0.bn1.running_mean[FLOAT, 480]\n",
      "  %blocks.4.0.bn1.running_var[FLOAT, 480]\n",
      "  %blocks.4.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.4.0.conv_dw.weight[FLOAT, 480x1x3x3]\n",
      "  %blocks.4.0.bn2.weight[FLOAT, 480]\n",
      "  %blocks.4.0.bn2.bias[FLOAT, 480]\n",
      "  %blocks.4.0.bn2.running_mean[FLOAT, 480]\n",
      "  %blocks.4.0.bn2.running_var[FLOAT, 480]\n",
      "  %blocks.4.0.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.4.0.se.conv_reduce.weight[FLOAT, 120x480x1x1]\n",
      "  %blocks.4.0.se.conv_reduce.bias[FLOAT, 120]\n",
      "  %blocks.4.0.se.conv_expand.weight[FLOAT, 480x120x1x1]\n",
      "  %blocks.4.0.se.conv_expand.bias[FLOAT, 480]\n",
      "  %blocks.4.0.conv_pwl.weight[FLOAT, 112x480x1x1]\n",
      "  %blocks.4.0.bn3.weight[FLOAT, 112]\n",
      "  %blocks.4.0.bn3.bias[FLOAT, 112]\n",
      "  %blocks.4.0.bn3.running_mean[FLOAT, 112]\n",
      "  %blocks.4.0.bn3.running_var[FLOAT, 112]\n",
      "  %blocks.4.0.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.4.1.conv_pw.weight[FLOAT, 672x112x1x1]\n",
      "  %blocks.4.1.bn1.weight[FLOAT, 672]\n",
      "  %blocks.4.1.bn1.bias[FLOAT, 672]\n",
      "  %blocks.4.1.bn1.running_mean[FLOAT, 672]\n",
      "  %blocks.4.1.bn1.running_var[FLOAT, 672]\n",
      "  %blocks.4.1.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.4.1.conv_dw.weight[FLOAT, 672x1x3x3]\n",
      "  %blocks.4.1.bn2.weight[FLOAT, 672]\n",
      "  %blocks.4.1.bn2.bias[FLOAT, 672]\n",
      "  %blocks.4.1.bn2.running_mean[FLOAT, 672]\n",
      "  %blocks.4.1.bn2.running_var[FLOAT, 672]\n",
      "  %blocks.4.1.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.4.1.se.conv_reduce.weight[FLOAT, 168x672x1x1]\n",
      "  %blocks.4.1.se.conv_reduce.bias[FLOAT, 168]\n",
      "  %blocks.4.1.se.conv_expand.weight[FLOAT, 672x168x1x1]\n",
      "  %blocks.4.1.se.conv_expand.bias[FLOAT, 672]\n",
      "  %blocks.4.1.conv_pwl.weight[FLOAT, 112x672x1x1]\n",
      "  %blocks.4.1.bn3.weight[FLOAT, 112]\n",
      "  %blocks.4.1.bn3.bias[FLOAT, 112]\n",
      "  %blocks.4.1.bn3.running_mean[FLOAT, 112]\n",
      "  %blocks.4.1.bn3.running_var[FLOAT, 112]\n",
      "  %blocks.4.1.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.0.conv_pw.weight[FLOAT, 672x112x1x1]\n",
      "  %blocks.5.0.bn1.weight[FLOAT, 672]\n",
      "  %blocks.5.0.bn1.bias[FLOAT, 672]\n",
      "  %blocks.5.0.bn1.running_mean[FLOAT, 672]\n",
      "  %blocks.5.0.bn1.running_var[FLOAT, 672]\n",
      "  %blocks.5.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.0.conv_dw.weight[FLOAT, 672x1x5x5]\n",
      "  %blocks.5.0.bn2.weight[FLOAT, 672]\n",
      "  %blocks.5.0.bn2.bias[FLOAT, 672]\n",
      "  %blocks.5.0.bn2.running_mean[FLOAT, 672]\n",
      "  %blocks.5.0.bn2.running_var[FLOAT, 672]\n",
      "  %blocks.5.0.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.0.se.conv_reduce.weight[FLOAT, 168x672x1x1]\n",
      "  %blocks.5.0.se.conv_reduce.bias[FLOAT, 168]\n",
      "  %blocks.5.0.se.conv_expand.weight[FLOAT, 672x168x1x1]\n",
      "  %blocks.5.0.se.conv_expand.bias[FLOAT, 672]\n",
      "  %blocks.5.0.conv_pwl.weight[FLOAT, 160x672x1x1]\n",
      "  %blocks.5.0.bn3.weight[FLOAT, 160]\n",
      "  %blocks.5.0.bn3.bias[FLOAT, 160]\n",
      "  %blocks.5.0.bn3.running_mean[FLOAT, 160]\n",
      "  %blocks.5.0.bn3.running_var[FLOAT, 160]\n",
      "  %blocks.5.0.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.1.conv_pw.weight[FLOAT, 960x160x1x1]\n",
      "  %blocks.5.1.bn1.weight[FLOAT, 960]\n",
      "  %blocks.5.1.bn1.bias[FLOAT, 960]\n",
      "  %blocks.5.1.bn1.running_mean[FLOAT, 960]\n",
      "  %blocks.5.1.bn1.running_var[FLOAT, 960]\n",
      "  %blocks.5.1.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.1.conv_dw.weight[FLOAT, 960x1x5x5]\n",
      "  %blocks.5.1.bn2.weight[FLOAT, 960]\n",
      "  %blocks.5.1.bn2.bias[FLOAT, 960]\n",
      "  %blocks.5.1.bn2.running_mean[FLOAT, 960]\n",
      "  %blocks.5.1.bn2.running_var[FLOAT, 960]\n",
      "  %blocks.5.1.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.1.se.conv_reduce.weight[FLOAT, 240x960x1x1]\n",
      "  %blocks.5.1.se.conv_reduce.bias[FLOAT, 240]\n",
      "  %blocks.5.1.se.conv_expand.weight[FLOAT, 960x240x1x1]\n",
      "  %blocks.5.1.se.conv_expand.bias[FLOAT, 960]\n",
      "  %blocks.5.1.conv_pwl.weight[FLOAT, 160x960x1x1]\n",
      "  %blocks.5.1.bn3.weight[FLOAT, 160]\n",
      "  %blocks.5.1.bn3.bias[FLOAT, 160]\n",
      "  %blocks.5.1.bn3.running_mean[FLOAT, 160]\n",
      "  %blocks.5.1.bn3.running_var[FLOAT, 160]\n",
      "  %blocks.5.1.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.2.conv_pw.weight[FLOAT, 960x160x1x1]\n",
      "  %blocks.5.2.bn1.weight[FLOAT, 960]\n",
      "  %blocks.5.2.bn1.bias[FLOAT, 960]\n",
      "  %blocks.5.2.bn1.running_mean[FLOAT, 960]\n",
      "  %blocks.5.2.bn1.running_var[FLOAT, 960]\n",
      "  %blocks.5.2.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.2.conv_dw.weight[FLOAT, 960x1x5x5]\n",
      "  %blocks.5.2.bn2.weight[FLOAT, 960]\n",
      "  %blocks.5.2.bn2.bias[FLOAT, 960]\n",
      "  %blocks.5.2.bn2.running_mean[FLOAT, 960]\n",
      "  %blocks.5.2.bn2.running_var[FLOAT, 960]\n",
      "  %blocks.5.2.bn2.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.5.2.se.conv_reduce.weight[FLOAT, 240x960x1x1]\n",
      "  %blocks.5.2.se.conv_reduce.bias[FLOAT, 240]\n",
      "  %blocks.5.2.se.conv_expand.weight[FLOAT, 960x240x1x1]\n",
      "  %blocks.5.2.se.conv_expand.bias[FLOAT, 960]\n",
      "  %blocks.5.2.conv_pwl.weight[FLOAT, 160x960x1x1]\n",
      "  %blocks.5.2.bn3.weight[FLOAT, 160]\n",
      "  %blocks.5.2.bn3.bias[FLOAT, 160]\n",
      "  %blocks.5.2.bn3.running_mean[FLOAT, 160]\n",
      "  %blocks.5.2.bn3.running_var[FLOAT, 160]\n",
      "  %blocks.5.2.bn3.num_batches_tracked[INT64, scalar]\n",
      "  %blocks.6.0.conv.weight[FLOAT, 960x160x1x1]\n",
      "  %blocks.6.0.bn1.weight[FLOAT, 960]\n",
      "  %blocks.6.0.bn1.bias[FLOAT, 960]\n",
      "  %blocks.6.0.bn1.running_mean[FLOAT, 960]\n",
      "  %blocks.6.0.bn1.running_var[FLOAT, 960]\n",
      "  %blocks.6.0.bn1.num_batches_tracked[INT64, scalar]\n",
      "  %conv_head.weight[FLOAT, 1280x960x1x1]\n",
      "  %classifier.weight[FLOAT, 1000x1280]\n",
      "  %classifier.bias[FLOAT, 1000]\n",
      ") {\n",
      "  %312 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%input.1, %conv_stem.weight)\n",
      "  %313 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%312, %bn1.weight, %bn1.bias, %bn1.running_mean, %bn1.running_var)\n",
      "  %314 = Constant[value = <Scalar Tensor []>]()\n",
      "  %315 = Add(%313, %314)\n",
      "  %316 = Clip[max = 6, min = 0](%315)\n",
      "  %317 = Constant[value = <Scalar Tensor []>]()\n",
      "  %318 = Div(%316, %317)\n",
      "  %319 = Mul(%313, %318)\n",
      "  %320 = Conv[dilations = [1, 1], group = 16, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%319, %blocks.0.0.conv_dw.weight)\n",
      "  %321 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%320, %blocks.0.0.bn1.weight, %blocks.0.0.bn1.bias, %blocks.0.0.bn1.running_mean, %blocks.0.0.bn1.running_var)\n",
      "  %322 = Relu(%321)\n",
      "  %323 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%322, %blocks.0.0.conv_pw.weight)\n",
      "  %324 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%323, %blocks.0.0.bn2.weight, %blocks.0.0.bn2.bias, %blocks.0.0.bn2.running_mean, %blocks.0.0.bn2.running_var)\n",
      "  %325 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%324, %blocks.1.0.conv_pw.weight)\n",
      "  %326 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%325, %blocks.1.0.bn1.weight, %blocks.1.0.bn1.bias, %blocks.1.0.bn1.running_mean, %blocks.1.0.bn1.running_var)\n",
      "  %327 = Relu(%326)\n",
      "  %328 = Conv[dilations = [1, 1], group = 64, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%327, %blocks.1.0.conv_dw.weight)\n",
      "  %329 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%328, %blocks.1.0.bn2.weight, %blocks.1.0.bn2.bias, %blocks.1.0.bn2.running_mean, %blocks.1.0.bn2.running_var)\n",
      "  %330 = Relu(%329)\n",
      "  %331 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%330, %blocks.1.0.conv_pwl.weight)\n",
      "  %332 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%331, %blocks.1.0.bn3.weight, %blocks.1.0.bn3.bias, %blocks.1.0.bn3.running_mean, %blocks.1.0.bn3.running_var)\n",
      "  %333 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%332, %blocks.1.1.conv_pw.weight)\n",
      "  %334 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%333, %blocks.1.1.bn1.weight, %blocks.1.1.bn1.bias, %blocks.1.1.bn1.running_mean, %blocks.1.1.bn1.running_var)\n",
      "  %335 = Relu(%334)\n",
      "  %336 = Conv[dilations = [1, 1], group = 72, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%335, %blocks.1.1.conv_dw.weight)\n",
      "  %337 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%336, %blocks.1.1.bn2.weight, %blocks.1.1.bn2.bias, %blocks.1.1.bn2.running_mean, %blocks.1.1.bn2.running_var)\n",
      "  %338 = Relu(%337)\n",
      "  %339 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%338, %blocks.1.1.conv_pwl.weight)\n",
      "  %340 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%339, %blocks.1.1.bn3.weight, %blocks.1.1.bn3.bias, %blocks.1.1.bn3.running_mean, %blocks.1.1.bn3.running_var)\n",
      "  %341 = Add(%340, %332)\n",
      "  %342 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%341, %blocks.2.0.conv_pw.weight)\n",
      "  %343 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%342, %blocks.2.0.bn1.weight, %blocks.2.0.bn1.bias, %blocks.2.0.bn1.running_mean, %blocks.2.0.bn1.running_var)\n",
      "  %344 = Relu(%343)\n",
      "  %345 = Conv[dilations = [1, 1], group = 72, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [2, 2]](%344, %blocks.2.0.conv_dw.weight)\n",
      "  %346 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%345, %blocks.2.0.bn2.weight, %blocks.2.0.bn2.bias, %blocks.2.0.bn2.running_mean, %blocks.2.0.bn2.running_var)\n",
      "  %347 = Relu(%346)\n",
      "  %348 = Constant[value = <Scalar Tensor []>]()\n",
      "  %349 = Shape(%347)\n",
      "  %350 = Gather[axis = 0](%349, %348)\n",
      "  %351 = Constant[value = <Scalar Tensor []>]()\n",
      "  %352 = Shape(%347)\n",
      "  %353 = Gather[axis = 0](%352, %351)\n",
      "  %354 = Constant[value = <Scalar Tensor []>]()\n",
      "  %355 = Unsqueeze[axes = [0]](%350)\n",
      "  %356 = Unsqueeze[axes = [0]](%353)\n",
      "  %357 = Unsqueeze[axes = [0]](%354)\n",
      "  %358 = Concat[axis = 0](%355, %356, %357)\n",
      "  %359 = Reshape(%347, %358)\n",
      "  %360 = ReduceMean[axes = [-1], keepdims = 0](%359)\n",
      "  %361 = Constant[value = <Scalar Tensor []>]()\n",
      "  %362 = Shape(%347)\n",
      "  %363 = Gather[axis = 0](%362, %361)\n",
      "  %364 = Constant[value = <Scalar Tensor []>]()\n",
      "  %365 = Shape(%347)\n",
      "  %366 = Gather[axis = 0](%365, %364)\n",
      "  %367 = Constant[value = <Scalar Tensor []>]()\n",
      "  %368 = Constant[value = <Scalar Tensor []>]()\n",
      "  %369 = Unsqueeze[axes = [0]](%363)\n",
      "  %370 = Unsqueeze[axes = [0]](%366)\n",
      "  %371 = Unsqueeze[axes = [0]](%367)\n",
      "  %372 = Unsqueeze[axes = [0]](%368)\n",
      "  %373 = Concat[axis = 0](%369, %370, %371, %372)\n",
      "  %374 = Reshape(%360, %373)\n",
      "  %375 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%374, %blocks.2.0.se.conv_reduce.weight, %blocks.2.0.se.conv_reduce.bias)\n",
      "  %376 = Relu(%375)\n",
      "  %377 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%376, %blocks.2.0.se.conv_expand.weight, %blocks.2.0.se.conv_expand.bias)\n",
      "  %378 = Constant[value = <Scalar Tensor []>]()\n",
      "  %379 = Add(%377, %378)\n",
      "  %380 = Clip[max = 6, min = 0](%379)\n",
      "  %381 = Constant[value = <Scalar Tensor []>]()\n",
      "  %382 = Div(%380, %381)\n",
      "  %383 = Mul(%347, %382)\n",
      "  %384 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%383, %blocks.2.0.conv_pwl.weight)\n",
      "  %385 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%384, %blocks.2.0.bn3.weight, %blocks.2.0.bn3.bias, %blocks.2.0.bn3.running_mean, %blocks.2.0.bn3.running_var)\n",
      "  %386 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%385, %blocks.2.1.conv_pw.weight)\n",
      "  %387 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%386, %blocks.2.1.bn1.weight, %blocks.2.1.bn1.bias, %blocks.2.1.bn1.running_mean, %blocks.2.1.bn1.running_var)\n",
      "  %388 = Relu(%387)\n",
      "  %389 = Conv[dilations = [1, 1], group = 120, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%388, %blocks.2.1.conv_dw.weight)\n",
      "  %390 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%389, %blocks.2.1.bn2.weight, %blocks.2.1.bn2.bias, %blocks.2.1.bn2.running_mean, %blocks.2.1.bn2.running_var)\n",
      "  %391 = Relu(%390)\n",
      "  %392 = Constant[value = <Scalar Tensor []>]()\n",
      "  %393 = Shape(%391)\n",
      "  %394 = Gather[axis = 0](%393, %392)\n",
      "  %395 = Constant[value = <Scalar Tensor []>]()\n",
      "  %396 = Shape(%391)\n",
      "  %397 = Gather[axis = 0](%396, %395)\n",
      "  %398 = Constant[value = <Scalar Tensor []>]()\n",
      "  %399 = Unsqueeze[axes = [0]](%394)\n",
      "  %400 = Unsqueeze[axes = [0]](%397)\n",
      "  %401 = Unsqueeze[axes = [0]](%398)\n",
      "  %402 = Concat[axis = 0](%399, %400, %401)\n",
      "  %403 = Reshape(%391, %402)\n",
      "  %404 = ReduceMean[axes = [-1], keepdims = 0](%403)\n",
      "  %405 = Constant[value = <Scalar Tensor []>]()\n",
      "  %406 = Shape(%391)\n",
      "  %407 = Gather[axis = 0](%406, %405)\n",
      "  %408 = Constant[value = <Scalar Tensor []>]()\n",
      "  %409 = Shape(%391)\n",
      "  %410 = Gather[axis = 0](%409, %408)\n",
      "  %411 = Constant[value = <Scalar Tensor []>]()\n",
      "  %412 = Constant[value = <Scalar Tensor []>]()\n",
      "  %413 = Unsqueeze[axes = [0]](%407)\n",
      "  %414 = Unsqueeze[axes = [0]](%410)\n",
      "  %415 = Unsqueeze[axes = [0]](%411)\n",
      "  %416 = Unsqueeze[axes = [0]](%412)\n",
      "  %417 = Concat[axis = 0](%413, %414, %415, %416)\n",
      "  %418 = Reshape(%404, %417)\n",
      "  %419 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%418, %blocks.2.1.se.conv_reduce.weight, %blocks.2.1.se.conv_reduce.bias)\n",
      "  %420 = Relu(%419)\n",
      "  %421 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%420, %blocks.2.1.se.conv_expand.weight, %blocks.2.1.se.conv_expand.bias)\n",
      "  %422 = Constant[value = <Scalar Tensor []>]()\n",
      "  %423 = Add(%421, %422)\n",
      "  %424 = Clip[max = 6, min = 0](%423)\n",
      "  %425 = Constant[value = <Scalar Tensor []>]()\n",
      "  %426 = Div(%424, %425)\n",
      "  %427 = Mul(%391, %426)\n",
      "  %428 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%427, %blocks.2.1.conv_pwl.weight)\n",
      "  %429 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%428, %blocks.2.1.bn3.weight, %blocks.2.1.bn3.bias, %blocks.2.1.bn3.running_mean, %blocks.2.1.bn3.running_var)\n",
      "  %430 = Add(%429, %385)\n",
      "  %431 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%430, %blocks.2.2.conv_pw.weight)\n",
      "  %432 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%431, %blocks.2.2.bn1.weight, %blocks.2.2.bn1.bias, %blocks.2.2.bn1.running_mean, %blocks.2.2.bn1.running_var)\n",
      "  %433 = Relu(%432)\n",
      "  %434 = Conv[dilations = [1, 1], group = 120, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%433, %blocks.2.2.conv_dw.weight)\n",
      "  %435 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%434, %blocks.2.2.bn2.weight, %blocks.2.2.bn2.bias, %blocks.2.2.bn2.running_mean, %blocks.2.2.bn2.running_var)\n",
      "  %436 = Relu(%435)\n",
      "  %437 = Constant[value = <Scalar Tensor []>]()\n",
      "  %438 = Shape(%436)\n",
      "  %439 = Gather[axis = 0](%438, %437)\n",
      "  %440 = Constant[value = <Scalar Tensor []>]()\n",
      "  %441 = Shape(%436)\n",
      "  %442 = Gather[axis = 0](%441, %440)\n",
      "  %443 = Constant[value = <Scalar Tensor []>]()\n",
      "  %444 = Unsqueeze[axes = [0]](%439)\n",
      "  %445 = Unsqueeze[axes = [0]](%442)\n",
      "  %446 = Unsqueeze[axes = [0]](%443)\n",
      "  %447 = Concat[axis = 0](%444, %445, %446)\n",
      "  %448 = Reshape(%436, %447)\n",
      "  %449 = ReduceMean[axes = [-1], keepdims = 0](%448)\n",
      "  %450 = Constant[value = <Scalar Tensor []>]()\n",
      "  %451 = Shape(%436)\n",
      "  %452 = Gather[axis = 0](%451, %450)\n",
      "  %453 = Constant[value = <Scalar Tensor []>]()\n",
      "  %454 = Shape(%436)\n",
      "  %455 = Gather[axis = 0](%454, %453)\n",
      "  %456 = Constant[value = <Scalar Tensor []>]()\n",
      "  %457 = Constant[value = <Scalar Tensor []>]()\n",
      "  %458 = Unsqueeze[axes = [0]](%452)\n",
      "  %459 = Unsqueeze[axes = [0]](%455)\n",
      "  %460 = Unsqueeze[axes = [0]](%456)\n",
      "  %461 = Unsqueeze[axes = [0]](%457)\n",
      "  %462 = Concat[axis = 0](%458, %459, %460, %461)\n",
      "  %463 = Reshape(%449, %462)\n",
      "  %464 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%463, %blocks.2.2.se.conv_reduce.weight, %blocks.2.2.se.conv_reduce.bias)\n",
      "  %465 = Relu(%464)\n",
      "  %466 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%465, %blocks.2.2.se.conv_expand.weight, %blocks.2.2.se.conv_expand.bias)\n",
      "  %467 = Constant[value = <Scalar Tensor []>]()\n",
      "  %468 = Add(%466, %467)\n",
      "  %469 = Clip[max = 6, min = 0](%468)\n",
      "  %470 = Constant[value = <Scalar Tensor []>]()\n",
      "  %471 = Div(%469, %470)\n",
      "  %472 = Mul(%436, %471)\n",
      "  %473 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%472, %blocks.2.2.conv_pwl.weight)\n",
      "  %474 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%473, %blocks.2.2.bn3.weight, %blocks.2.2.bn3.bias, %blocks.2.2.bn3.running_mean, %blocks.2.2.bn3.running_var)\n",
      "  %475 = Add(%474, %430)\n",
      "  %476 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%475, %blocks.3.0.conv_pw.weight)\n",
      "  %477 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%476, %blocks.3.0.bn1.weight, %blocks.3.0.bn1.bias, %blocks.3.0.bn1.running_mean, %blocks.3.0.bn1.running_var)\n",
      "  %478 = Constant[value = <Scalar Tensor []>]()\n",
      "  %479 = Add(%477, %478)\n",
      "  %480 = Clip[max = 6, min = 0](%479)\n",
      "  %481 = Constant[value = <Scalar Tensor []>]()\n",
      "  %482 = Div(%480, %481)\n",
      "  %483 = Mul(%477, %482)\n",
      "  %484 = Conv[dilations = [1, 1], group = 240, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%483, %blocks.3.0.conv_dw.weight)\n",
      "  %485 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%484, %blocks.3.0.bn2.weight, %blocks.3.0.bn2.bias, %blocks.3.0.bn2.running_mean, %blocks.3.0.bn2.running_var)\n",
      "  %486 = Constant[value = <Scalar Tensor []>]()\n",
      "  %487 = Add(%485, %486)\n",
      "  %488 = Clip[max = 6, min = 0](%487)\n",
      "  %489 = Constant[value = <Scalar Tensor []>]()\n",
      "  %490 = Div(%488, %489)\n",
      "  %491 = Mul(%485, %490)\n",
      "  %492 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%491, %blocks.3.0.conv_pwl.weight)\n",
      "  %493 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%492, %blocks.3.0.bn3.weight, %blocks.3.0.bn3.bias, %blocks.3.0.bn3.running_mean, %blocks.3.0.bn3.running_var)\n",
      "  %494 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%493, %blocks.3.1.conv_pw.weight)\n",
      "  %495 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%494, %blocks.3.1.bn1.weight, %blocks.3.1.bn1.bias, %blocks.3.1.bn1.running_mean, %blocks.3.1.bn1.running_var)\n",
      "  %496 = Constant[value = <Scalar Tensor []>]()\n",
      "  %497 = Add(%495, %496)\n",
      "  %498 = Clip[max = 6, min = 0](%497)\n",
      "  %499 = Constant[value = <Scalar Tensor []>]()\n",
      "  %500 = Div(%498, %499)\n",
      "  %501 = Mul(%495, %500)\n",
      "  %502 = Conv[dilations = [1, 1], group = 200, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%501, %blocks.3.1.conv_dw.weight)\n",
      "  %503 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%502, %blocks.3.1.bn2.weight, %blocks.3.1.bn2.bias, %blocks.3.1.bn2.running_mean, %blocks.3.1.bn2.running_var)\n",
      "  %504 = Constant[value = <Scalar Tensor []>]()\n",
      "  %505 = Add(%503, %504)\n",
      "  %506 = Clip[max = 6, min = 0](%505)\n",
      "  %507 = Constant[value = <Scalar Tensor []>]()\n",
      "  %508 = Div(%506, %507)\n",
      "  %509 = Mul(%503, %508)\n",
      "  %510 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%509, %blocks.3.1.conv_pwl.weight)\n",
      "  %511 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%510, %blocks.3.1.bn3.weight, %blocks.3.1.bn3.bias, %blocks.3.1.bn3.running_mean, %blocks.3.1.bn3.running_var)\n",
      "  %512 = Add(%511, %493)\n",
      "  %513 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%512, %blocks.3.2.conv_pw.weight)\n",
      "  %514 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%513, %blocks.3.2.bn1.weight, %blocks.3.2.bn1.bias, %blocks.3.2.bn1.running_mean, %blocks.3.2.bn1.running_var)\n",
      "  %515 = Constant[value = <Scalar Tensor []>]()\n",
      "  %516 = Add(%514, %515)\n",
      "  %517 = Clip[max = 6, min = 0](%516)\n",
      "  %518 = Constant[value = <Scalar Tensor []>]()\n",
      "  %519 = Div(%517, %518)\n",
      "  %520 = Mul(%514, %519)\n",
      "  %521 = Conv[dilations = [1, 1], group = 184, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%520, %blocks.3.2.conv_dw.weight)\n",
      "  %522 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%521, %blocks.3.2.bn2.weight, %blocks.3.2.bn2.bias, %blocks.3.2.bn2.running_mean, %blocks.3.2.bn2.running_var)\n",
      "  %523 = Constant[value = <Scalar Tensor []>]()\n",
      "  %524 = Add(%522, %523)\n",
      "  %525 = Clip[max = 6, min = 0](%524)\n",
      "  %526 = Constant[value = <Scalar Tensor []>]()\n",
      "  %527 = Div(%525, %526)\n",
      "  %528 = Mul(%522, %527)\n",
      "  %529 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%528, %blocks.3.2.conv_pwl.weight)\n",
      "  %530 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%529, %blocks.3.2.bn3.weight, %blocks.3.2.bn3.bias, %blocks.3.2.bn3.running_mean, %blocks.3.2.bn3.running_var)\n",
      "  %531 = Add(%530, %512)\n",
      "  %532 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%531, %blocks.3.3.conv_pw.weight)\n",
      "  %533 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%532, %blocks.3.3.bn1.weight, %blocks.3.3.bn1.bias, %blocks.3.3.bn1.running_mean, %blocks.3.3.bn1.running_var)\n",
      "  %534 = Constant[value = <Scalar Tensor []>]()\n",
      "  %535 = Add(%533, %534)\n",
      "  %536 = Clip[max = 6, min = 0](%535)\n",
      "  %537 = Constant[value = <Scalar Tensor []>]()\n",
      "  %538 = Div(%536, %537)\n",
      "  %539 = Mul(%533, %538)\n",
      "  %540 = Conv[dilations = [1, 1], group = 184, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%539, %blocks.3.3.conv_dw.weight)\n",
      "  %541 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%540, %blocks.3.3.bn2.weight, %blocks.3.3.bn2.bias, %blocks.3.3.bn2.running_mean, %blocks.3.3.bn2.running_var)\n",
      "  %542 = Constant[value = <Scalar Tensor []>]()\n",
      "  %543 = Add(%541, %542)\n",
      "  %544 = Clip[max = 6, min = 0](%543)\n",
      "  %545 = Constant[value = <Scalar Tensor []>]()\n",
      "  %546 = Div(%544, %545)\n",
      "  %547 = Mul(%541, %546)\n",
      "  %548 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%547, %blocks.3.3.conv_pwl.weight)\n",
      "  %549 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%548, %blocks.3.3.bn3.weight, %blocks.3.3.bn3.bias, %blocks.3.3.bn3.running_mean, %blocks.3.3.bn3.running_var)\n",
      "  %550 = Add(%549, %531)\n",
      "  %551 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%550, %blocks.4.0.conv_pw.weight)\n",
      "  %552 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%551, %blocks.4.0.bn1.weight, %blocks.4.0.bn1.bias, %blocks.4.0.bn1.running_mean, %blocks.4.0.bn1.running_var)\n",
      "  %553 = Constant[value = <Scalar Tensor []>]()\n",
      "  %554 = Add(%552, %553)\n",
      "  %555 = Clip[max = 6, min = 0](%554)\n",
      "  %556 = Constant[value = <Scalar Tensor []>]()\n",
      "  %557 = Div(%555, %556)\n",
      "  %558 = Mul(%552, %557)\n",
      "  %559 = Conv[dilations = [1, 1], group = 480, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%558, %blocks.4.0.conv_dw.weight)\n",
      "  %560 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%559, %blocks.4.0.bn2.weight, %blocks.4.0.bn2.bias, %blocks.4.0.bn2.running_mean, %blocks.4.0.bn2.running_var)\n",
      "  %561 = Constant[value = <Scalar Tensor []>]()\n",
      "  %562 = Add(%560, %561)\n",
      "  %563 = Clip[max = 6, min = 0](%562)\n",
      "  %564 = Constant[value = <Scalar Tensor []>]()\n",
      "  %565 = Div(%563, %564)\n",
      "  %566 = Mul(%560, %565)\n",
      "  %567 = Constant[value = <Scalar Tensor []>]()\n",
      "  %568 = Shape(%566)\n",
      "  %569 = Gather[axis = 0](%568, %567)\n",
      "  %570 = Constant[value = <Scalar Tensor []>]()\n",
      "  %571 = Shape(%566)\n",
      "  %572 = Gather[axis = 0](%571, %570)\n",
      "  %573 = Constant[value = <Scalar Tensor []>]()\n",
      "  %574 = Unsqueeze[axes = [0]](%569)\n",
      "  %575 = Unsqueeze[axes = [0]](%572)\n",
      "  %576 = Unsqueeze[axes = [0]](%573)\n",
      "  %577 = Concat[axis = 0](%574, %575, %576)\n",
      "  %578 = Reshape(%566, %577)\n",
      "  %579 = ReduceMean[axes = [-1], keepdims = 0](%578)\n",
      "  %580 = Constant[value = <Scalar Tensor []>]()\n",
      "  %581 = Shape(%566)\n",
      "  %582 = Gather[axis = 0](%581, %580)\n",
      "  %583 = Constant[value = <Scalar Tensor []>]()\n",
      "  %584 = Shape(%566)\n",
      "  %585 = Gather[axis = 0](%584, %583)\n",
      "  %586 = Constant[value = <Scalar Tensor []>]()\n",
      "  %587 = Constant[value = <Scalar Tensor []>]()\n",
      "  %588 = Unsqueeze[axes = [0]](%582)\n",
      "  %589 = Unsqueeze[axes = [0]](%585)\n",
      "  %590 = Unsqueeze[axes = [0]](%586)\n",
      "  %591 = Unsqueeze[axes = [0]](%587)\n",
      "  %592 = Concat[axis = 0](%588, %589, %590, %591)\n",
      "  %593 = Reshape(%579, %592)\n",
      "  %594 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%593, %blocks.4.0.se.conv_reduce.weight, %blocks.4.0.se.conv_reduce.bias)\n",
      "  %595 = Constant[value = <Scalar Tensor []>]()\n",
      "  %596 = Add(%594, %595)\n",
      "  %597 = Clip[max = 6, min = 0](%596)\n",
      "  %598 = Constant[value = <Scalar Tensor []>]()\n",
      "  %599 = Div(%597, %598)\n",
      "  %600 = Mul(%594, %599)\n",
      "  %601 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%600, %blocks.4.0.se.conv_expand.weight, %blocks.4.0.se.conv_expand.bias)\n",
      "  %602 = Constant[value = <Scalar Tensor []>]()\n",
      "  %603 = Add(%601, %602)\n",
      "  %604 = Clip[max = 6, min = 0](%603)\n",
      "  %605 = Constant[value = <Scalar Tensor []>]()\n",
      "  %606 = Div(%604, %605)\n",
      "  %607 = Mul(%566, %606)\n",
      "  %608 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%607, %blocks.4.0.conv_pwl.weight)\n",
      "  %609 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%608, %blocks.4.0.bn3.weight, %blocks.4.0.bn3.bias, %blocks.4.0.bn3.running_mean, %blocks.4.0.bn3.running_var)\n",
      "  %610 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%609, %blocks.4.1.conv_pw.weight)\n",
      "  %611 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%610, %blocks.4.1.bn1.weight, %blocks.4.1.bn1.bias, %blocks.4.1.bn1.running_mean, %blocks.4.1.bn1.running_var)\n",
      "  %612 = Constant[value = <Scalar Tensor []>]()\n",
      "  %613 = Add(%611, %612)\n",
      "  %614 = Clip[max = 6, min = 0](%613)\n",
      "  %615 = Constant[value = <Scalar Tensor []>]()\n",
      "  %616 = Div(%614, %615)\n",
      "  %617 = Mul(%611, %616)\n",
      "  %618 = Conv[dilations = [1, 1], group = 672, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%617, %blocks.4.1.conv_dw.weight)\n",
      "  %619 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%618, %blocks.4.1.bn2.weight, %blocks.4.1.bn2.bias, %blocks.4.1.bn2.running_mean, %blocks.4.1.bn2.running_var)\n",
      "  %620 = Constant[value = <Scalar Tensor []>]()\n",
      "  %621 = Add(%619, %620)\n",
      "  %622 = Clip[max = 6, min = 0](%621)\n",
      "  %623 = Constant[value = <Scalar Tensor []>]()\n",
      "  %624 = Div(%622, %623)\n",
      "  %625 = Mul(%619, %624)\n",
      "  %626 = Constant[value = <Scalar Tensor []>]()\n",
      "  %627 = Shape(%625)\n",
      "  %628 = Gather[axis = 0](%627, %626)\n",
      "  %629 = Constant[value = <Scalar Tensor []>]()\n",
      "  %630 = Shape(%625)\n",
      "  %631 = Gather[axis = 0](%630, %629)\n",
      "  %632 = Constant[value = <Scalar Tensor []>]()\n",
      "  %633 = Unsqueeze[axes = [0]](%628)\n",
      "  %634 = Unsqueeze[axes = [0]](%631)\n",
      "  %635 = Unsqueeze[axes = [0]](%632)\n",
      "  %636 = Concat[axis = 0](%633, %634, %635)\n",
      "  %637 = Reshape(%625, %636)\n",
      "  %638 = ReduceMean[axes = [-1], keepdims = 0](%637)\n",
      "  %639 = Constant[value = <Scalar Tensor []>]()\n",
      "  %640 = Shape(%625)\n",
      "  %641 = Gather[axis = 0](%640, %639)\n",
      "  %642 = Constant[value = <Scalar Tensor []>]()\n",
      "  %643 = Shape(%625)\n",
      "  %644 = Gather[axis = 0](%643, %642)\n",
      "  %645 = Constant[value = <Scalar Tensor []>]()\n",
      "  %646 = Constant[value = <Scalar Tensor []>]()\n",
      "  %647 = Unsqueeze[axes = [0]](%641)\n",
      "  %648 = Unsqueeze[axes = [0]](%644)\n",
      "  %649 = Unsqueeze[axes = [0]](%645)\n",
      "  %650 = Unsqueeze[axes = [0]](%646)\n",
      "  %651 = Concat[axis = 0](%647, %648, %649, %650)\n",
      "  %652 = Reshape(%638, %651)\n",
      "  %653 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%652, %blocks.4.1.se.conv_reduce.weight, %blocks.4.1.se.conv_reduce.bias)\n",
      "  %654 = Constant[value = <Scalar Tensor []>]()\n",
      "  %655 = Add(%653, %654)\n",
      "  %656 = Clip[max = 6, min = 0](%655)\n",
      "  %657 = Constant[value = <Scalar Tensor []>]()\n",
      "  %658 = Div(%656, %657)\n",
      "  %659 = Mul(%653, %658)\n",
      "  %660 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%659, %blocks.4.1.se.conv_expand.weight, %blocks.4.1.se.conv_expand.bias)\n",
      "  %661 = Constant[value = <Scalar Tensor []>]()\n",
      "  %662 = Add(%660, %661)\n",
      "  %663 = Clip[max = 6, min = 0](%662)\n",
      "  %664 = Constant[value = <Scalar Tensor []>]()\n",
      "  %665 = Div(%663, %664)\n",
      "  %666 = Mul(%625, %665)\n",
      "  %667 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%666, %blocks.4.1.conv_pwl.weight)\n",
      "  %668 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%667, %blocks.4.1.bn3.weight, %blocks.4.1.bn3.bias, %blocks.4.1.bn3.running_mean, %blocks.4.1.bn3.running_var)\n",
      "  %669 = Add(%668, %609)\n",
      "  %670 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%669, %blocks.5.0.conv_pw.weight)\n",
      "  %671 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%670, %blocks.5.0.bn1.weight, %blocks.5.0.bn1.bias, %blocks.5.0.bn1.running_mean, %blocks.5.0.bn1.running_var)\n",
      "  %672 = Constant[value = <Scalar Tensor []>]()\n",
      "  %673 = Add(%671, %672)\n",
      "  %674 = Clip[max = 6, min = 0](%673)\n",
      "  %675 = Constant[value = <Scalar Tensor []>]()\n",
      "  %676 = Div(%674, %675)\n",
      "  %677 = Mul(%671, %676)\n",
      "  %678 = Conv[dilations = [1, 1], group = 672, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [2, 2]](%677, %blocks.5.0.conv_dw.weight)\n",
      "  %679 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%678, %blocks.5.0.bn2.weight, %blocks.5.0.bn2.bias, %blocks.5.0.bn2.running_mean, %blocks.5.0.bn2.running_var)\n",
      "  %680 = Constant[value = <Scalar Tensor []>]()\n",
      "  %681 = Add(%679, %680)\n",
      "  %682 = Clip[max = 6, min = 0](%681)\n",
      "  %683 = Constant[value = <Scalar Tensor []>]()\n",
      "  %684 = Div(%682, %683)\n",
      "  %685 = Mul(%679, %684)\n",
      "  %686 = Constant[value = <Scalar Tensor []>]()\n",
      "  %687 = Shape(%685)\n",
      "  %688 = Gather[axis = 0](%687, %686)\n",
      "  %689 = Constant[value = <Scalar Tensor []>]()\n",
      "  %690 = Shape(%685)\n",
      "  %691 = Gather[axis = 0](%690, %689)\n",
      "  %692 = Constant[value = <Scalar Tensor []>]()\n",
      "  %693 = Unsqueeze[axes = [0]](%688)\n",
      "  %694 = Unsqueeze[axes = [0]](%691)\n",
      "  %695 = Unsqueeze[axes = [0]](%692)\n",
      "  %696 = Concat[axis = 0](%693, %694, %695)\n",
      "  %697 = Reshape(%685, %696)\n",
      "  %698 = ReduceMean[axes = [-1], keepdims = 0](%697)\n",
      "  %699 = Constant[value = <Scalar Tensor []>]()\n",
      "  %700 = Shape(%685)\n",
      "  %701 = Gather[axis = 0](%700, %699)\n",
      "  %702 = Constant[value = <Scalar Tensor []>]()\n",
      "  %703 = Shape(%685)\n",
      "  %704 = Gather[axis = 0](%703, %702)\n",
      "  %705 = Constant[value = <Scalar Tensor []>]()\n",
      "  %706 = Constant[value = <Scalar Tensor []>]()\n",
      "  %707 = Unsqueeze[axes = [0]](%701)\n",
      "  %708 = Unsqueeze[axes = [0]](%704)\n",
      "  %709 = Unsqueeze[axes = [0]](%705)\n",
      "  %710 = Unsqueeze[axes = [0]](%706)\n",
      "  %711 = Concat[axis = 0](%707, %708, %709, %710)\n",
      "  %712 = Reshape(%698, %711)\n",
      "  %713 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%712, %blocks.5.0.se.conv_reduce.weight, %blocks.5.0.se.conv_reduce.bias)\n",
      "  %714 = Constant[value = <Scalar Tensor []>]()\n",
      "  %715 = Add(%713, %714)\n",
      "  %716 = Clip[max = 6, min = 0](%715)\n",
      "  %717 = Constant[value = <Scalar Tensor []>]()\n",
      "  %718 = Div(%716, %717)\n",
      "  %719 = Mul(%713, %718)\n",
      "  %720 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%719, %blocks.5.0.se.conv_expand.weight, %blocks.5.0.se.conv_expand.bias)\n",
      "  %721 = Constant[value = <Scalar Tensor []>]()\n",
      "  %722 = Add(%720, %721)\n",
      "  %723 = Clip[max = 6, min = 0](%722)\n",
      "  %724 = Constant[value = <Scalar Tensor []>]()\n",
      "  %725 = Div(%723, %724)\n",
      "  %726 = Mul(%685, %725)\n",
      "  %727 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%726, %blocks.5.0.conv_pwl.weight)\n",
      "  %728 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%727, %blocks.5.0.bn3.weight, %blocks.5.0.bn3.bias, %blocks.5.0.bn3.running_mean, %blocks.5.0.bn3.running_var)\n",
      "  %729 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%728, %blocks.5.1.conv_pw.weight)\n",
      "  %730 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%729, %blocks.5.1.bn1.weight, %blocks.5.1.bn1.bias, %blocks.5.1.bn1.running_mean, %blocks.5.1.bn1.running_var)\n",
      "  %731 = Constant[value = <Scalar Tensor []>]()\n",
      "  %732 = Add(%730, %731)\n",
      "  %733 = Clip[max = 6, min = 0](%732)\n",
      "  %734 = Constant[value = <Scalar Tensor []>]()\n",
      "  %735 = Div(%733, %734)\n",
      "  %736 = Mul(%730, %735)\n",
      "  %737 = Conv[dilations = [1, 1], group = 960, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%736, %blocks.5.1.conv_dw.weight)\n",
      "  %738 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%737, %blocks.5.1.bn2.weight, %blocks.5.1.bn2.bias, %blocks.5.1.bn2.running_mean, %blocks.5.1.bn2.running_var)\n",
      "  %739 = Constant[value = <Scalar Tensor []>]()\n",
      "  %740 = Add(%738, %739)\n",
      "  %741 = Clip[max = 6, min = 0](%740)\n",
      "  %742 = Constant[value = <Scalar Tensor []>]()\n",
      "  %743 = Div(%741, %742)\n",
      "  %744 = Mul(%738, %743)\n",
      "  %745 = Constant[value = <Scalar Tensor []>]()\n",
      "  %746 = Shape(%744)\n",
      "  %747 = Gather[axis = 0](%746, %745)\n",
      "  %748 = Constant[value = <Scalar Tensor []>]()\n",
      "  %749 = Shape(%744)\n",
      "  %750 = Gather[axis = 0](%749, %748)\n",
      "  %751 = Constant[value = <Scalar Tensor []>]()\n",
      "  %752 = Unsqueeze[axes = [0]](%747)\n",
      "  %753 = Unsqueeze[axes = [0]](%750)\n",
      "  %754 = Unsqueeze[axes = [0]](%751)\n",
      "  %755 = Concat[axis = 0](%752, %753, %754)\n",
      "  %756 = Reshape(%744, %755)\n",
      "  %757 = ReduceMean[axes = [-1], keepdims = 0](%756)\n",
      "  %758 = Constant[value = <Scalar Tensor []>]()\n",
      "  %759 = Shape(%744)\n",
      "  %760 = Gather[axis = 0](%759, %758)\n",
      "  %761 = Constant[value = <Scalar Tensor []>]()\n",
      "  %762 = Shape(%744)\n",
      "  %763 = Gather[axis = 0](%762, %761)\n",
      "  %764 = Constant[value = <Scalar Tensor []>]()\n",
      "  %765 = Constant[value = <Scalar Tensor []>]()\n",
      "  %766 = Unsqueeze[axes = [0]](%760)\n",
      "  %767 = Unsqueeze[axes = [0]](%763)\n",
      "  %768 = Unsqueeze[axes = [0]](%764)\n",
      "  %769 = Unsqueeze[axes = [0]](%765)\n",
      "  %770 = Concat[axis = 0](%766, %767, %768, %769)\n",
      "  %771 = Reshape(%757, %770)\n",
      "  %772 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%771, %blocks.5.1.se.conv_reduce.weight, %blocks.5.1.se.conv_reduce.bias)\n",
      "  %773 = Constant[value = <Scalar Tensor []>]()\n",
      "  %774 = Add(%772, %773)\n",
      "  %775 = Clip[max = 6, min = 0](%774)\n",
      "  %776 = Constant[value = <Scalar Tensor []>]()\n",
      "  %777 = Div(%775, %776)\n",
      "  %778 = Mul(%772, %777)\n",
      "  %779 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%778, %blocks.5.1.se.conv_expand.weight, %blocks.5.1.se.conv_expand.bias)\n",
      "  %780 = Constant[value = <Scalar Tensor []>]()\n",
      "  %781 = Add(%779, %780)\n",
      "  %782 = Clip[max = 6, min = 0](%781)\n",
      "  %783 = Constant[value = <Scalar Tensor []>]()\n",
      "  %784 = Div(%782, %783)\n",
      "  %785 = Mul(%744, %784)\n",
      "  %786 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%785, %blocks.5.1.conv_pwl.weight)\n",
      "  %787 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%786, %blocks.5.1.bn3.weight, %blocks.5.1.bn3.bias, %blocks.5.1.bn3.running_mean, %blocks.5.1.bn3.running_var)\n",
      "  %788 = Add(%787, %728)\n",
      "  %789 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%788, %blocks.5.2.conv_pw.weight)\n",
      "  %790 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%789, %blocks.5.2.bn1.weight, %blocks.5.2.bn1.bias, %blocks.5.2.bn1.running_mean, %blocks.5.2.bn1.running_var)\n",
      "  %791 = Constant[value = <Scalar Tensor []>]()\n",
      "  %792 = Add(%790, %791)\n",
      "  %793 = Clip[max = 6, min = 0](%792)\n",
      "  %794 = Constant[value = <Scalar Tensor []>]()\n",
      "  %795 = Div(%793, %794)\n",
      "  %796 = Mul(%790, %795)\n",
      "  %797 = Conv[dilations = [1, 1], group = 960, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%796, %blocks.5.2.conv_dw.weight)\n",
      "  %798 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%797, %blocks.5.2.bn2.weight, %blocks.5.2.bn2.bias, %blocks.5.2.bn2.running_mean, %blocks.5.2.bn2.running_var)\n",
      "  %799 = Constant[value = <Scalar Tensor []>]()\n",
      "  %800 = Add(%798, %799)\n",
      "  %801 = Clip[max = 6, min = 0](%800)\n",
      "  %802 = Constant[value = <Scalar Tensor []>]()\n",
      "  %803 = Div(%801, %802)\n",
      "  %804 = Mul(%798, %803)\n",
      "  %805 = Constant[value = <Scalar Tensor []>]()\n",
      "  %806 = Shape(%804)\n",
      "  %807 = Gather[axis = 0](%806, %805)\n",
      "  %808 = Constant[value = <Scalar Tensor []>]()\n",
      "  %809 = Shape(%804)\n",
      "  %810 = Gather[axis = 0](%809, %808)\n",
      "  %811 = Constant[value = <Scalar Tensor []>]()\n",
      "  %812 = Unsqueeze[axes = [0]](%807)\n",
      "  %813 = Unsqueeze[axes = [0]](%810)\n",
      "  %814 = Unsqueeze[axes = [0]](%811)\n",
      "  %815 = Concat[axis = 0](%812, %813, %814)\n",
      "  %816 = Reshape(%804, %815)\n",
      "  %817 = ReduceMean[axes = [-1], keepdims = 0](%816)\n",
      "  %818 = Constant[value = <Scalar Tensor []>]()\n",
      "  %819 = Shape(%804)\n",
      "  %820 = Gather[axis = 0](%819, %818)\n",
      "  %821 = Constant[value = <Scalar Tensor []>]()\n",
      "  %822 = Shape(%804)\n",
      "  %823 = Gather[axis = 0](%822, %821)\n",
      "  %824 = Constant[value = <Scalar Tensor []>]()\n",
      "  %825 = Constant[value = <Scalar Tensor []>]()\n",
      "  %826 = Unsqueeze[axes = [0]](%820)\n",
      "  %827 = Unsqueeze[axes = [0]](%823)\n",
      "  %828 = Unsqueeze[axes = [0]](%824)\n",
      "  %829 = Unsqueeze[axes = [0]](%825)\n",
      "  %830 = Concat[axis = 0](%826, %827, %828, %829)\n",
      "  %831 = Reshape(%817, %830)\n",
      "  %832 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%831, %blocks.5.2.se.conv_reduce.weight, %blocks.5.2.se.conv_reduce.bias)\n",
      "  %833 = Constant[value = <Scalar Tensor []>]()\n",
      "  %834 = Add(%832, %833)\n",
      "  %835 = Clip[max = 6, min = 0](%834)\n",
      "  %836 = Constant[value = <Scalar Tensor []>]()\n",
      "  %837 = Div(%835, %836)\n",
      "  %838 = Mul(%832, %837)\n",
      "  %839 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%838, %blocks.5.2.se.conv_expand.weight, %blocks.5.2.se.conv_expand.bias)\n",
      "  %840 = Constant[value = <Scalar Tensor []>]()\n",
      "  %841 = Add(%839, %840)\n",
      "  %842 = Clip[max = 6, min = 0](%841)\n",
      "  %843 = Constant[value = <Scalar Tensor []>]()\n",
      "  %844 = Div(%842, %843)\n",
      "  %845 = Mul(%804, %844)\n",
      "  %846 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%845, %blocks.5.2.conv_pwl.weight)\n",
      "  %847 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%846, %blocks.5.2.bn3.weight, %blocks.5.2.bn3.bias, %blocks.5.2.bn3.running_mean, %blocks.5.2.bn3.running_var)\n",
      "  %848 = Add(%847, %788)\n",
      "  %849 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%848, %blocks.6.0.conv.weight)\n",
      "  %850 = BatchNormalization[epsilon = 0.00100000004749745, momentum = 0.899999976158142](%849, %blocks.6.0.bn1.weight, %blocks.6.0.bn1.bias, %blocks.6.0.bn1.running_mean, %blocks.6.0.bn1.running_var)\n",
      "  %851 = Constant[value = <Scalar Tensor []>]()\n",
      "  %852 = Add(%850, %851)\n",
      "  %853 = Clip[max = 6, min = 0](%852)\n",
      "  %854 = Constant[value = <Scalar Tensor []>]()\n",
      "  %855 = Div(%853, %854)\n",
      "  %856 = Mul(%850, %855)\n",
      "  %857 = GlobalAveragePool(%856)\n",
      "  %858 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%857, %conv_head.weight)\n",
      "  %859 = Constant[value = <Scalar Tensor []>]()\n",
      "  %860 = Add(%858, %859)\n",
      "  %861 = Clip[max = 6, min = 0](%860)\n",
      "  %862 = Constant[value = <Scalar Tensor []>]()\n",
      "  %863 = Div(%861, %862)\n",
      "  %864 = Mul(%858, %863)\n",
      "  %865 = Constant[value = <Scalar Tensor []>]()\n",
      "  %866 = Shape(%864)\n",
      "  %867 = Gather[axis = 0](%866, %865)\n",
      "  %868 = Constant[value = <Scalar Tensor []>]()\n",
      "  %869 = Unsqueeze[axes = [0]](%867)\n",
      "  %870 = Unsqueeze[axes = [0]](%868)\n",
      "  %871 = Concat[axis = 0](%869, %870)\n",
      "  %872 = Reshape(%864, %871)\n",
      "  %873 = Gemm[alpha = 1, beta = 1, transB = 1](%872, %classifier.weight, %classifier.bias)\n",
      "  return %873\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"mobile.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model expects input shape: [1, 3, 224, 224]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession('mobile.onnx')\n",
    "\n",
    "print(\"The model expects input shape:\", sess.get_inputs()[0].shape)\n",
    "# print(\"image shape:\", ximg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from pytorch2keras import pytorch_to_keras\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "# we should specify shape of the input tensor\n",
    "# k_model = pytorch_to_keras(m,input_shapes=dummy_input, verbose=True)  \n",
    "input_np = np.random.uniform(0, 1, (1,3, 224, 224))\n",
    "input_var = Variable(torch.FloatTensor(input_np))\n",
    "k_model = pytorch_to_keras(m, input_var, [(3,224,224)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
