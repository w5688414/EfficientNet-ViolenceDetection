{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from sklearn import svm\n",
    "from scipy.stats import mode\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./data/raw_frames/hocky/fi1_xvid'\n",
    "pickle_name='video_summary.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(path,pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images_path': 'data/raw_frames/hocky/fi1_xvid',\n",
       " 'name': 'fi1_xvid',\n",
       " 'images_files': ['data/raw_frames/hocky/fi1_xvid/frame_0.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_2.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_4.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_6.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_8.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_10.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_12.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_14.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_16.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_18.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_20.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_22.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_24.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_26.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_28.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_30.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_32.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_34.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_36.jpg',\n",
       "  'data/raw_frames/hocky/fi1_xvid/frame_38.jpg'],\n",
       " 'sequence_length': 41,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/raw_frames/hocky/fi1_xvid/frame_0.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_2.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_4.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_6.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_8.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_10.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_12.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_14.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_16.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_18.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_20.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_22.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_24.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_26.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_28.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_30.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_32.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_34.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_36.jpg',\n",
       " 'data/raw_frames/hocky/fi1_xvid/frame_38.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['images_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresLimit = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtraction( videoPath, actionName, type):\n",
    "\n",
    "\n",
    "    # Set frame path, if jpeg directory doesn't exist , take images from video dir\n",
    "    framePath = videoPath\n",
    "    if os.path.exists( framePath + \"/jpeg\") :\n",
    "        framePath += \"/jpeg/\"\n",
    "\n",
    "    # Extract feature\n",
    "#     imageFrames = getImageList(framePath)\n",
    "\n",
    "    pickle_name='video_summary.pkl'\n",
    "    file_path=os.path.join(videoPath,pickle_name)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        imageFrames=data['images_files']\n",
    "    #print \"DEBUG: Image Frames - \", imageFrames\n",
    "\n",
    "    frameCount = 0\n",
    "    frameIndex = 0\n",
    "\n",
    "    # Feature List for a video\n",
    "    videoFeatures  = []\n",
    "\n",
    "    for iFrame in imageFrames:\n",
    "\n",
    "        frameIndex += 1\n",
    "\n",
    "        # Read Frame\n",
    "        frame = cv2.imread(iFrame)\n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # HOG Descriptor , default value it takes window size= 64x128, block size= 16x16, block stride= 8x8, cell size= 8x8, bins= 9\n",
    "        hogDescriptor = cv2.HOGDescriptor()\n",
    "\n",
    "        # Returns histogram\n",
    "        hist = hogDescriptor.compute(gray)\n",
    "\n",
    "        #sortedHogDescriptor = hogDescriptor\n",
    "        sortedHogHist = np.sort(hist, axis=None)\n",
    "\n",
    "        keyFeatures = sortedHogHist[- featuresLimit : ]\n",
    "\n",
    "        if type == \"Trng\":\n",
    "            keyFeatures = np.insert(keyFeatures, 0, sportsActionTag[actionName])\n",
    "\n",
    "        videoFeatures.append(keyFeatures)\n",
    "\n",
    "        # Lowest number of frame available in a video\n",
    "        if frameCount >= 23:\n",
    "            break\n",
    "\n",
    "        frameCount += 1\n",
    "\n",
    "\n",
    "    return videoFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageList(imageDirectory):\n",
    "\n",
    "    # Find different type of images\n",
    "    rImages = glob.glob(imageDirectory + \"/*.jpg\")\n",
    "    rImages +=  glob.glob(imageDirectory + \"/*.jpeg\")\n",
    "    rImages +=  glob.glob(imageDirectory + \"/*.png\")\n",
    "\n",
    "    return rImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportsActionTag = {\n",
    "    'no_fight': 0,\n",
    "    'fight':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath='./data/raw_frames/hocky/fi1_xvid'\n",
    "sportsActionName='fight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFeatures = featureExtraction(videoPath , sportsActionName, 'Trng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(videoFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.62515384 0.62515384 ... 0.9804249  0.9804249  0.9804249 ]\n"
     ]
    }
   ],
   "source": [
    "print(videoFeatures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfDir(path):\n",
    "    # Read each sport action directory\n",
    "    dirs  = os.listdir(path)\n",
    "\n",
    "    sportsActionsCount = 0\n",
    "    filtered_dir  = []\n",
    "    # Remove . .. and hidden directory\n",
    "    for dir in dirs:\n",
    "        if not dir.startswith(\".\"):\n",
    "            filtered_dir.append(dir)\n",
    "\n",
    "    return filtered_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportsActionPath='./data/raw_frames/hocky'\n",
    "sportsActionList = getListOfDir( sportsActionPath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fi100_xvid', 'fi101_xvid', 'fi102_xvid', 'fi103_xvid', 'fi104_xvid']\n"
     ]
    }
   ],
   "source": [
    "print(sportsActionList[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [9:53:31<00:00,  5.27s/it]    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "firstActionFlag = 0\n",
    "videoFeatures = []\n",
    "videoCount=1\n",
    "for sportsActionName in tqdm(sportsActionList):\n",
    "#     if(videoCount==10):\n",
    "#         break\n",
    "    sportsActionDir = sportsActionPath + \"/\" + sportsActionName\n",
    "    if(sportsActionName.startswith(\"fi\")):\n",
    "        sportsActionName='fight'\n",
    "    elif(sportsActionName.startswith(\"no\")):\n",
    "        sportsActionName='no_fight'\n",
    "        \n",
    "    videoFeatures = featureExtraction(sportsActionDir , sportsActionName, 'Trng')\n",
    "#     print(len(videoFeatures))\n",
    "    # Put together all the videos\n",
    "    if firstActionFlag == 0:\n",
    "        sportsActionFeatures = videoFeatures\n",
    "        firstActionFlag = 1\n",
    "    else:\n",
    "        sportsActionFeatures = np.concatenate( (sportsActionFeatures, videoFeatures), axis=0)\n",
    "\n",
    "    videoCount += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(sportsActionFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sportsActionFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for featureAndLabel in tqdm(sportsActionFeatures):\n",
    "#     label.append(int(featureAndLabel[0]))\n",
    "#     feature.append((np.delete(featureAndLabel, 0)).tolist())\n",
    "# print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(label[-5:])\n",
    "sportsActionNumber = len(sportsActionTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation( truth, predicted, categoryIndex ):\n",
    "\n",
    "    # TP,FP,FN,TN indicate True Positive, False Positive, False Negative, True Negative respectively\n",
    "    TP = 1\n",
    "    FP = 1\n",
    "    FN = 1\n",
    "    TN = 1\n",
    "\n",
    "    # Categories are Sports Action 1=>0, Sports Action 2=> 1, Sports Action 3=>2  etc..\n",
    "    for fIndex in range(len(truth)):\n",
    "\n",
    "         # Positive prediction for each feature\n",
    "        if ( int(predicted[fIndex]) == categoryIndex):\n",
    "            # TP=> when P[i] = T[i] = Ci\n",
    "            if (int(truth[fIndex]) == int (predicted[fIndex])):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else: # Negative Prediction\n",
    "            if ( int ( truth[fIndex]) == categoryIndex ):\n",
    "                FN += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "\n",
    "\n",
    "    # Calculate Sensitivity - True Positive Rate - Recall\n",
    "    sensitivity = TP / float ( TP + FN )\n",
    "\n",
    "    # Specificity - True Negative Rate\n",
    "    specificity = TN / float ( TN + FP )\n",
    "\n",
    "    #Calculate accuracy\n",
    "    accuracy =  ( TP + TN ) / float ( TP + FP + FN + TN )\n",
    "\n",
    "\n",
    "    return sensitivity, specificity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSportsActionName(saIndex):\n",
    "\n",
    "    keys   = sportsActionTag.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        if saIndex == sportsActionTag[key]:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation( featureAndLabelList):\n",
    "\n",
    "    # Randomize the sample\n",
    "    np.random.shuffle(featureAndLabelList)\n",
    "\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    sensitivity = 0.0\n",
    "    specificity = 0.0\n",
    "    accuracy    = 0.0\n",
    "\n",
    "\n",
    "    # split feature set in equal subsets same as number of sports actions for cross validation\n",
    "#     subsetLength =  len(featureAndLabelList) / sportsActionNumber\n",
    "    subsetLength =  int(len(featureAndLabelList)*0.8)\n",
    "    for rIndex in range(sportsActionNumber):\n",
    "\n",
    "        print(\"INFO: Cross Validation Iteration - \", rIndex)\n",
    "        trainigSet   = []\n",
    "        valdationSet = []\n",
    "        feature = []\n",
    "        label   = []\n",
    "\n",
    "\n",
    "        if ( rIndex == 0 ):\n",
    "            trainigSet = featureAndLabelList[1*subsetLength:]\n",
    "            valdationSet = featureAndLabelList[0: subsetLength]\n",
    "        elif ( rIndex == (sportsActionNumber -1) ):\n",
    "            trainigSet = featureAndLabelList[:(sportsActionNumber -1)*subsetLength]\n",
    "            valdationSet = featureAndLabelList[(sportsActionNumber -1)*subsetLength : ]\n",
    "        else:\n",
    "            trainigSet = np.concatenate ((featureAndLabelList[:rIndex * subsetLength] , featureAndLabelList[(rIndex + 1) * subsetLength: ]), axis=0 )\n",
    "            valdationSet = featureAndLabelList[rIndex * subsetLength : (rIndex + 1 ) * subsetLength]\n",
    "\n",
    "        # Get all features in a array\n",
    "        for featureAndLabel in trainigSet:\n",
    "            label.append(int(featureAndLabel[0]))\n",
    "            feature.append((np.delete(featureAndLabel, 0)).tolist())\n",
    "\n",
    "\n",
    "        # Train model\n",
    "        print(\"INFO: Training ... \")\n",
    "        clf=RandomForestClassifier(n_estimators=13)\n",
    "        clf=clf.fit(feature,label)\n",
    "\n",
    "        # Prepare validation feature and label to be predicted\n",
    "        print(\"INFO: Prediction for \", getSportsActionName(rIndex))\n",
    "        vFeatureList = []\n",
    "        vLabelList   = [] # Ground Truth\n",
    "        for featureAndLabel in valdationSet:\n",
    "            vFeatureList.append(featureAndLabel[1:].tolist())\n",
    "            vLabelList.append(featureAndLabel[0])\n",
    "\n",
    "        # Predict the class label for Validation Feature List\n",
    "        predictedLabel = clf.predict(vFeatureList)\n",
    "\n",
    "        # predict validation set and calculate accuracy\n",
    "        print(\"INFO: Evaluating ... \")\n",
    "\n",
    "        # Evaluation < Truth>, <Predicted>, <Sports Action Index>\n",
    "        (sen, spec , accu ) = evaluation(vLabelList , predictedLabel.tolist() , rIndex)\n",
    "\n",
    "        sensitivity += sen\n",
    "        specificity += spec\n",
    "        accuracy    += accu\n",
    "\n",
    "        print(\"\\t   Sensitivity : \", sen)\n",
    "        print(\"\\t   Specificity : \", spec)\n",
    "        print(\"\\t   Accuracy    : \", accu)\n",
    "\n",
    "\n",
    "    # Average evaluation metrics\n",
    "    avgSensitivity = sensitivity / sportsActionNumber\n",
    "    avgSpecificity = specificity / sportsActionNumber\n",
    "    avgAccuracy = accuracy / sportsActionNumber\n",
    "\n",
    "\n",
    "    print(\"  *** Overall Evaluation ***\")\n",
    "    print(\"    Average Sensitivity: \", avgSensitivity)\n",
    "    print(\"    Average Specificity: \", avgSpecificity)\n",
    "    print(\"    Average Accuracy   : \", avgAccuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20032, 15001)\n"
     ]
    }
   ],
   "source": [
    "print(sportsActionFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidation(sportsActionFeatures)\n",
    "featureAndLabelList=sportsActionFeatures\n",
    "np.random.shuffle(featureAndLabelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetLength =  int(len(featureAndLabelList)*0.8)\n",
    "trainigSet = featureAndLabelList[:subsetLength]\n",
    "valdationSet = featureAndLabelList[subsetLength : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all features in a array\n",
    "label = []\n",
    "feature = []\n",
    "for featureAndLabel in trainigSet:\n",
    "    label.append(int(featureAndLabel[0]))\n",
    "    feature.append((np.delete(featureAndLabel, 0)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Training ... \n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"INFO: Training ... \")\n",
    "clf=RandomForestClassifier(n_estimators=13)\n",
    "clf=clf.fit(feature,label)\n",
    "\n",
    "# Prepare validation feature and label to be predicted\n",
    "# print(\"INFO: Prediction for \", getSportsActionName(rIndex))\n",
    "vFeatureList = []\n",
    "vLabelList   = [] # Ground Truth\n",
    "for featureAndLabel in valdationSet:\n",
    "    vFeatureList.append(featureAndLabel[1:].tolist())\n",
    "    vLabelList.append(featureAndLabel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Evaluating ... \n"
     ]
    }
   ],
   "source": [
    "# Predict the class label for Validation Feature List\n",
    "predictedLabel = clf.predict(vFeatureList)\n",
    "\n",
    "# predict validation set and calculate accuracy\n",
    "print(\"INFO: Evaluating ... \")\n",
    "\n",
    "# Evaluation < Truth>, <Predicted>, <Sports Action Index>\n",
    "(sen, spec , accu ) = evaluation(vLabelList , predictedLabel.tolist() , 0)\n",
    "(sen1, spec1 , accu1 ) = evaluation(vLabelList , predictedLabel.tolist() , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *** Overall Evaluation ***\n",
      "    Average Sensitivity:  0.7569041874262317\n",
      "    Average Specificity:  0.7569041874262317\n",
      "    Average Accuracy   :  0.7571677885814011\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "sensitivity = 0.0\n",
    "specificity = 0.0\n",
    "accuracy    = 0.0\n",
    "\n",
    "sensitivity += sen\n",
    "specificity += spec\n",
    "accuracy    += accu\n",
    "\n",
    "sensitivity += sen1\n",
    "specificity += spec1\n",
    "accuracy    += accu1\n",
    "# print(\"\\t   Sensitivity : \", sen)\n",
    "# print(\"\\t   Specificity : \", spec)\n",
    "# print(\"\\t   Accuracy    : \", accu)\n",
    "# Average evaluation metrics\n",
    "avgSensitivity = sensitivity / sportsActionNumber\n",
    "avgSpecificity = specificity / sportsActionNumber\n",
    "avgAccuracy = accuracy / sportsActionNumber\n",
    "\n",
    "\n",
    "print(\"  *** Overall Evaluation ***\")\n",
    "print(\"    Average Sensitivity: \", avgSensitivity)\n",
    "print(\"    Average Specificity: \", avgSpecificity)\n",
    "print(\"    Average Accuracy   : \", avgAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Training ... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=13, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"INFO: Training ... \")\n",
    "clf = svm.SVC(gamma=0.01, C=13)\n",
    "clf.fit(feature,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Evaluating ... \n"
     ]
    }
   ],
   "source": [
    "# Predict the class label for Validation Feature List\n",
    "predictedLabel = clf.predict(vFeatureList)\n",
    "\n",
    "# predict validation set and calculate accuracy\n",
    "print(\"INFO: Evaluating ... \")\n",
    "\n",
    "# Evaluation < Truth>, <Predicted>, <Sports Action Index>\n",
    "(sen, spec , accu ) = evaluation(vLabelList , predictedLabel.tolist() , 0)\n",
    "(sen1, spec1 , accu1 ) = evaluation(vLabelList , predictedLabel.tolist() , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *** Overall Evaluation ***\n",
      "    Average Sensitivity:  0.7186830863075654\n",
      "    Average Specificity:  0.7186830863075654\n",
      "    Average Accuracy   :  0.7190226876090751\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "sensitivity = 0.0\n",
    "specificity = 0.0\n",
    "accuracy    = 0.0\n",
    "\n",
    "sensitivity += sen\n",
    "specificity += spec\n",
    "accuracy    += accu\n",
    "\n",
    "sensitivity += sen1\n",
    "specificity += spec1\n",
    "accuracy    += accu1\n",
    "# print(\"\\t   Sensitivity : \", sen)\n",
    "# print(\"\\t   Specificity : \", spec)\n",
    "# print(\"\\t   Accuracy    : \", accu)\n",
    "# Average evaluation metrics\n",
    "avgSensitivity = sensitivity / sportsActionNumber\n",
    "avgSpecificity = specificity / sportsActionNumber\n",
    "avgAccuracy = accuracy / sportsActionNumber\n",
    "\n",
    "\n",
    "print(\"  *** Overall Evaluation ***\")\n",
    "print(\"    Average Sensitivity: \", avgSensitivity)\n",
    "print(\"    Average Specificity: \", avgSpecificity)\n",
    "print(\"    Average Accuracy   : \", avgAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
